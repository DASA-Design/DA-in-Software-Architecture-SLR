@book{10.5555/1971981,
  title = {Developing High Quality Data Models},
  author = {West, Matthew},
  date = {2011},
  edition = {1},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  abstract = {A multitude of problems is likely to arise when developing data models. With dozens of attributes and millions of rows, data modelers are always in danger of inconsistency and inaccuracy. The development of the data model itself could result in difficulties presenting accurate data. The need to improve data models begins with getting it right in the first place. Using real-world examples, Developing High Quality Data Models walks the reader through identifying a number of data modeling principles and analysis techniques that enable the development of data models that both meet business requirements and have a consistent basis. The reader is presented with a variety of generic data model patterns that both exemplify the principles and techniques discussed and build upon one another to give a powerful and integrated generic data model. This model has wide applicability across many disciplines in government and industry, including but not limited to energy exploration, healthcare, telecommunications, transportation, military defense, transportation, and more. * Uses a number of common data model patterns to explain how to develop data models over a wide scope in a way that is consistent and of high quality *Offers generic data model templates that are reusable in many applications and are fundamental for developing more specific templates *Develops ideas for creating consistent approaches to high quality data models},
  isbn = {978-0-12-375106-5}
}

@book{10.5555/2597865,
  title = {Business Intelligence: {{The}} Savvy Manager's Guide},
  author = {Loshin, David},
  date = {2012},
  edition = {2},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  abstract = {Following the footsteps of the first edition, the second edition of Business Intelligence is a full overview of what comprises business intelligence. It is intended to provide an introduction to the concepts to uncomplicate the learning process when implementing a business intelligence program. Over a relatively long lifetime (7 years), the current edition of book has received numerous accolades from across the industry for its straightforward introduction to both business and technical aspects of business intelligence. As an author, David Loshin has a distinct ability to translate challenging topics into a framework that is easily digestible by managers, business analysts, and technologists alike. In addition, his material has developed a following (such as the recent Master Data Management book) among practitioners and key figures in the industry (both analysts and vendors) and that magnifies our ability to convey the value of this book. Guides managers through developing, administering, or simply understanding business intelligence technology Keeps pace with the changes in best practices, tools, methods and processes used to transform an organizations data into actionable knowledge Contains a handy, quick-reference to technologies and terminology. Table of Contents 1. Business Intelligence - An Introduction 2. Value Drivers 3. Planning for Success 4. Developing a Business Intelligence Roadmap 5. The Business Intelligence Environment 6. Business Models and Information Flow 7. Data Requirements Analysis 8. Data Warehouses and the Technical BI Architecture 9. Business Metadata 10. Data Profiling 11. Business Rules 12. Data Quality 13. Data Integration 14. High Performance BI 15. Alternate Information Contexts 16. Location Intelligence and Spatial analysis 17. Knowledge Discovery, Data Mining, and Analytics 18. Using Publicly Available Data 19. Knowledge Delivery 20. New and Emerging Techniques 21. Quick Reference},
  isbn = {978-0-12-385890-0}
}

@book{10.5555/2769773,
  title = {Big Data Analytics: {{From}} Strategic Planning to Enterprise Integration with Tools, Techniques, {{NoSQL}}, and Graph},
  author = {Loshin, David},
  date = {2013},
  publisher = {Morgan Kaufmann Publishers Inc.},
  location = {San Francisco, CA, USA},
  abstract = {Big Data Analytics will assist managers in providing an overview of the drivers for introducing big data technology into the organization and for understanding the types of business problems best suited to big data analytics solutions, understanding the value drivers and benefits, strategic planning, developing a pilot, and eventually planning to integrate back into production within the enterprise. Guides the reader in assessing the opportunities and value proposition Overview of big data hardware and software architectures Presents a variety of technologies and how they fit into the big data ecosystem Table of Contents Business Drivers for Big Data Analytics Business Problems that are Suited to Big Data Solutions Organizational Alignment for Big Data Developing a Strategic Plan for Integrating Big Data into the Enterprise Data Governance: Policies and Processes for Big Data Oversight High Performance Analytics Appliances Big Data Tools and Techniques Developing Big Data Applications NoSQL Graph Analytics Summary},
  isbn = {978-0-12-418664-4}
}

@article{basser_hybrid_2015,
  title = {Hybrid {{ANFIS}}–{{PSO}} Approach for Predicting Optimum Parameters of a Protective Spur Dike},
  author = {Basser, Hossein and Karami, Hojat and Shamshirband, Shahaboddin and Akib, Shatirah and Amirmojahedi, Mohsen and Ahmad, Rodina and Jahangirzadeh, Afshin and Javidnia, Hossein},
  date = {2015-05-01},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {30},
  pages = {642--649},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2015.02.011},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494615001015},
  abstract = {In this study a new approach was proposed to determine optimum parameters of a protective spur dike to mitigate scouring depth amount around existing main spur dikes. The studied parameters were angle of the protective spur dike relative to the flume wall, its length, and its distance from the main spur dikes, flow intensity, and the diameters of the sediment particles that were explored to find the optimum amounts. In prediction phase, a novel hybrid approach was developed, combining adaptive-network-based fuzzy inference system and particle swarm optimization (ANFIS–PSO) to predict protective spur dike's parameters in order to control scouring around a series of spur dikes. The results indicated that the accuracy of the proposed method is increased significantly compared to other approaches. In addition, the effectiveness of the developed method was confirmed using the available data.},
  keywords = {Neuro-fuzzy,Prediction,Scour,Swarm optimization}
}

@inproceedings{bodzay_aspectmatlab_2015,
  title = {{{AspectMatlab}}++: Annotations, Types, and Aspects for Scientists},
  booktitle = {Proceedings of the 14th International Conference on Modularity},
  author = {Bodzay, Andrew and Hendren, Laurie},
  date = {2015},
  series = {Modularity 2015},
  pages = {41--54},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2724525.2724573},
  url = {https://doi.org/10.1145/2724525.2724573},
  abstract = {In this paper we present extensions to an aspect oriented compiler developed for MATLAB. These extensions are intended to support important functionality for scientists, and include pattern matching on annotations, and types of variables, as well as new manners of exposing context. We provide use-cases of these features in the form of several general-use aspects which focus on solving issues that arise from use of dynamically-typed languages. We also detail performance enhancements to the ASPECTMATLAB compiler which result in an order of magnitude in performance gains.},
  isbn = {978-1-4503-3249-1},
  pagetotal = {14},
  keywords = {Aspect-oriented programming,ASPECTMATLAB compiler,MATLAB},
  file = {C:\Users\Felipe\Zotero\storage\P356ZZ7X\Bodzay and Hendren - 2015 - AspectMatlab++ annotations, types, and aspects for scientists.pdf}
}

@article{brace_coramod_2014,
  title = {{{CORAMOD}}: A Checklist-Oriented Model-Based Requirements Analysis Approach},
  shorttitle = {{{CORAMOD}}},
  author = {Brace, William and Ekman, Kalevi},
  date = {2014-03-01},
  journaltitle = {Requirements Engineering},
  shortjournal = {Requirements Eng},
  volume = {19},
  number = {1},
  pages = {1--26},
  issn = {1432-010X},
  doi = {10.1007/s00766-012-0154-3},
  url = {https://doi.org/10.1007/s00766-012-0154-3},
  urldate = {2024-07-16},
  abstract = {Requirement development activities such as requirements analysis and modelling are well defined in software engineering. A model-based requirement development may result in significant improvements in engineering design. In current product development activities in this domain, not all requirements are consciously identified and modelled. This paper presents the checklist-oriented requirements analysis modelling (CORAMOD) approach. CORAMOD is a methodology for the use of model-based systems engineering for requirements analysis of complex products utilizing checklists, the simplest kind of rational design method. The model-based focuses the requirements analysis process on requirement modelling, whereas the checklist encourages a conscious and systematic approach to identify requirements. We illustrate the utility of CORAMOD artefacts by a comprehensive case study example and modelling with system modelling language (SysML). We suggest that visual accessibility of the SysML views facilitates the full participation of all stakeholders and enables the necessary dialogue and negotiation. The approach promotes tracing derived requirements to the customer need statement and enhances validation by model execution and simulation.},
  langid = {english},
  keywords = {CORAMOD,Formalize requirement,Model-based,Model-based requirements analysis,Requirement models,Requirements analysis},
  file = {C:\Users\Felipe\Zotero\storage\GXV9NKHF\Brace and Ekman - 2014 - CORAMOD a checklist-oriented model-based requirem.pdf}
}

@article{cai_optimisation_2011,
  title = {The Optimisation Research of Screw Conveyor},
  author = {Cai, Jiang-hui and Meng, Wen-jun and Chen, Zhi-mei},
  date = {2011-08-01},
  journaltitle = {Int. J. Innov. Comput. Appl.},
  volume = {3},
  number = {3},
  pages = {169--176},
  issn = {1751-648X},
  doi = {10.1504/IJICA.2011.041918},
  url = {https://doi.org/10.1504/IJICA.2011.041918},
  urldate = {2025-07-09},
  abstract = {Particle swarm optimisation (PSO) is a population-based stochastic optimisation technique. As a result, PSO algorithm is widely used in mechanical engineering design field. Screw conveyors are used extensively in agriculture and processing industries for elevating and\&amp;\#47;or transporting bulk materials over short to medium distances. They are very effective for conveying dry particulate solids, giving good control over the throughput. Despite their apparent simplicity, the transportation action is very complex and designers have tended to rely heavily on empirical performance data. Intelligent operation parameters optimisation for screw conveyor based on PSO is studied in this paper. This thesis takes a heavy driving drum of screw conveyor as an example, firstly, the optimisation function is built up, then the operation parameter of screw conveyor is precision optimised with PSO algorithm, which offer a foundation to design more reasonable structure for driving drum in order to meet the application demands.}
}

@article{da_silva_sousa_methodology_2010,
  title = {Methodology for Automatic Detection of Lung Nodules in Computerized Tomography Images},
  author = {family=Silva Sousa, given=João Rodrigo Ferreira, prefix=da, useprefix=true and Silva, Aristófanes Corrěa and family=Paiva, given=Anselmo Cardoso, prefix=de, useprefix=true and Nunes, Rodolfo Acatauassú},
  date = {2010-04-01},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  shortjournal = {Computer Methods and Programs in Biomedicine},
  volume = {98},
  number = {1},
  pages = {1--14},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2009.07.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0169260709002235},
  abstract = {Lung cancer is a disease with significant prevalence in several countries around the world. Its difficult treatment and rapid progression make the mortality rates among people affected by this illness to be very high. Aiming to offer a computational alternative for helping in detection of nodules, serving as a second opinion to the specialists, this work proposes a totally automatic methodology based on successive detection refining stages. The automated lung nodules detection scheme consists of six stages: thorax extraction, lung extraction, lung reconstruction, structures extraction, tubular structures elimination, and false positive reduction. In the thorax extraction stage all the artifacts external to the patient’s body are discarded. Lung extraction stage is responsible for the identification of the lung parenchyma. The objective of the lung reconstruction stage is to prevent incorrect elimination of portions belonging to the parenchyma. Structures extraction stage comprises the selection of dense structures from inside the lung parenchyma. The next stage, tubular structures elimination eliminates a great part of the pulmonary trees. Finally, the false positive stage selects only structures with great probability to be nodule. Each of the several stages has very specific objectives in detection of particular cases of lung nodules, ensuring good matching rates even in difficult detection situations. We use 33 exams with diversified diagnosis and slices numbers for validating the methodology. We obtained a false positive per exam rate of 0.42 and false negative rate of 0.15. The total classification sensitivity obtained, measured out of the nodule candidates, was 84.84\%. The specificity achieved was 96.15\% and the total accuracy of the method was 95.21\%.},
  keywords = {Computer tomography (CT),Computer-aided detection (CAD),Image processing,Lung nodules,Medical image}
}

@article{dinesh_jackson_samuel_retracted_2019,
  title = {{{RETRACTED ARTICLE}}: {{Tuberculosis}} ({{TB}}) Detection System Using Deep Neural Networks},
  shorttitle = {{{RETRACTED ARTICLE}}},
  author = {Dinesh Jackson Samuel, R. and Rajesh Kanna, B.},
  date = {2019-05-01},
  journaltitle = {Neural Comput. Appl.},
  volume = {31},
  number = {5},
  pages = {1533--1545},
  issn = {0941-0643},
  doi = {10.1007/s00521-018-3564-4},
  url = {https://doi.org/10.1007/s00521-018-3564-4},
  urldate = {2025-07-09},
  abstract = {Microscopy is a rapid diagnosis method for many infectious diseases like tuberculosis (TB). In TB bacilli identification, specimens are stained using Ziehl–Neelsen or Auramine dye\&nbsp;and are examined by technicians thoroughly for any infectious microbes. For pathological study, the images of these microbes are captured using microscopes and image processing is applied for further analysis. However, choosing 100 field of views (FOV) randomly from a 2\,×\,1\&nbsp;cm square area of sputum specimen may lead to inconsistency in specificity. The examination of specimens is a tedious process, and it requires especially skilled technicians for screening the sputum smear samples. The proposed tuberculosis detection system consists of two subsystems—a data acquisition system and a recognition system. In the data acquisition system, a motorized microscopic stage is designed and developed to automate the acquisition of all FOVs. Here the microscopic stage movement is motorized and scanning patterns are defined by the user for specimen examination. After the acquisition of all FOVs, data are passed to the recognition system. In the recognition system, transfer learning method is implemented by customizing the Inception V3 DeepNet model. This model learns from the pre-trained weights of Inception V3 and classifies the data using support vector machine (SVM) from the transferred knowledge. For training and testing the customized Inception V3 model, a public TB dataset (Shah et al. in J Med Imaging 4(2):027503, 2017. https://doi-org.ezproxy.uniandes.edu.co/10.1117/1.jmi.4.2.027503) and our own acquired microscopic digital dataset are used for analysis. In this\&nbsp;model, the fixed feature representations are taken from the top stack layer of Inception V3 DeepNet and are classified using SVM. This model attains an accuracy of 95.05\%, thereby reducing the dependency on skilled technicians in the screening process and increasing sensitivity and specificity.}
}

@article{ding_planetary_2011,
  title = {Planetary Rovers’ Wheel–Soil Interaction Mechanics: New Challenges and Applications for Wheeled Mobile Robots},
  shorttitle = {Planetary Rovers’ Wheel–Soil Interaction Mechanics},
  author = {Ding, Liang and Deng, Zongquan and Gao, Haibo and Nagatani, Keiji and Yoshida, Kazuya},
  date = {2011-01-01},
  journaltitle = {Intelligent Service Robotics},
  shortjournal = {Intel Serv Robotics},
  volume = {4},
  number = {1},
  pages = {17--38},
  issn = {1861-2784},
  doi = {10.1007/s11370-010-0080-5},
  url = {https://doi.org/10.1007/s11370-010-0080-5},
  urldate = {2024-07-16},
  abstract = {With the increasing challenges facing planetary exploration missions and the resultant increase in the performance requirements for planetary rovers, terramechanics (wheel–soil interaction mechanics) is playing an important role in the development of these rovers. As an extension of the conventional terramechanics theory for terrestrial vehicles, the terramechanics theory for planetary rovers, which is becoming a new research hotspot, is unique and puts forward many new challenging problems. This paper first discusses the significance of the study of wheel–soil interaction mechanics of planetary rovers and summarizes the differences between planetary rovers and terrestrial vehicles and the problems arising thereof. The application of terramechanics to the development of planetary rovers can be divided into two phases (the R\&D phase and exploration phase for rovers) corresponding to the high-fidelity and simplified terramechanics models. This paper also describes the current research status by providing an introduction to classical terramechanics and the experimental, theoretical, and numerical researches on terramechanics for planetary rovers. The application status of the terramechanics for planetary rovers is analyzed from the aspects of rover design, performance evaluation, planetary soil parameter identification, dynamics simulation, mobility control, and path planning. Finally, the key issues for future research are discussed. The current planetary rovers are actually advanced wheeled mobile robots (WMRs), developed employing cutting-edge technologies from different fields. The terramechanics for planetary rovers is expected to present new challenges and applications for WMRs, making it possible to develop WMRs using the concepts of mechanics and dynamics.},
  langid = {english},
  keywords = {Control,Design and performance evaluation,Dynamics simulation,Planetary rover,Soil parameter identification,Terramechanics},
  file = {C:\Users\Felipe\Zotero\storage\C7NSK837\Ding et al. - 2011 - Planetary rovers’ wheel–soil interaction mechanics.pdf}
}

@article{edwin_raja_dhas_evolutionary_2016,
  title = {Evolutionary Fuzzy {{SVR}} Modeling of Weld Residual Stress},
  author = {Edwin Raja Dhas, J. and Kumanan, Somasundaram},
  date = {2016-05-01},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {42},
  pages = {423--430},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2016.01.050},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494616300382},
  abstract = {Residual stresses are an integral part of the total stress acting on any component in service. It is important to determine and/or predict the magnitude, nature and direction of the residual stress to estimate the life of important engineering parts, particularly welded components. Researchers have developed many direct measuring techniques for welding residual stress. Intelligent techniques have been developed to predict residual stresses to meet the demands of advanced manufacturing planning. This research paper explores the development of Finite Element model and evolutionary fuzzy support vector regression model for the prediction of residual stress in welding. Residual stress model is developed using Finite Element Simulation. Results from Finite Element Method (FEM) model are used to train and test the developed Fuzzy Support Vector Regression model tuned with Genetic Algorithm (FSVRGA) using K-fold cross validation method. The performance of the developed model is compared with Support Vector Regression model and Fuzzy Support Vector Regression model. The proposed and developed model is superior in terms of computational speed and accuracy. Developed models are validated and reported. The developed model finds scope in setting the initial weld process parameters.},
  keywords = {Genetic Algorithm,Manufacturing,Residual stress,Support Vector Regression,Welding,X-ray diffraction}
}

@article{elham_coupled_2016,
  title = {Coupled Adjoint Aerostructural Wing Optimization Using Quasi-Three-Dimensional Aerodynamic Analysis},
  author = {Elham, Ali and Tooren, Michel J.},
  date = {2016-10-01},
  journaltitle = {Struct. Multidiscip. Optim.},
  volume = {54},
  number = {4},
  pages = {889--906},
  issn = {1615-147X},
  doi = {10.1007/s00158-016-1447-9},
  url = {https://doi.org/10.1007/s00158-016-1447-9},
  urldate = {2025-07-09},
  abstract = {This paper presents a method for wing aerostructural analysis and optimization, which needs much lower computational costs, while computes the wing drag and structural deformation with a level of accuracy comparable to the higher fidelity CFD and FEM tools. A quasi-three-dimensional aerodynamic solver is developed and connected to a finite beam element model for wing aerostructural optimization. In a quasi-three-dimensional approach an inviscid incompressible vortex lattice method is coupled with a viscous compressible airfoil analysis code for drag prediction of a three dimensional wing. The accuracy of the proposed method for wing drag prediction is validated by comparing its results with the results of a higher fidelity CFD analysis. The wing structural deformation as well as the stress distribution in the wingbox structure is computed using a finite beam element model. The Newton method is used to solve the coupled system. The sensitivities of the outputs, for example the wing drag, with respect to the inputs, for example the wing geometry, is computed by a combined use of the coupled adjoint method, automatic differentiation and the chain rule of differentiation. A gradient based optimization is performed using the proposed tool for minimizing the fuel weight of an A320 class aircraft. The optimization resulted in more than 10 \% reduction in the aircraft fuel weight by optimizing the wing planform and airfoils shape as well as the wing internal structure.},
  file = {C:\Users\Felipe\Zotero\storage\6MJPVGKW\Elham and Tooren - 2016 - Coupled adjoint aerostructural wing optimization using quasi-three-dimensional aerodynamic analysis.pdf}
}

@article{fugger_reconciling_2012,
  title = {Reconciling Fault-Tolerant Distributed Computing and Systems-on-Chip},
  author = {Függer, Matthias and Schmid, Ulrich},
  date = {2012-01},
  journaltitle = {Distributed Computing},
  shortjournal = {Distrib. Comput.},
  volume = {24},
  number = {6},
  pages = {323--355},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  issn = {0178-2770},
  doi = {10.1007/s00446-011-0151-7},
  url = {https://doi.org/10.1007/s00446-011-0151-7},
  abstract = {Classic distributed computing abstractions do not match well the reality of digital logic gates, which are the elementary building blocks of Systems-on-Chip (SoCs) and other Very Large Scale Integrated (VLSI) circuits: Massively concurrent, continuous computations undermine the concept of sequential processes executing sequences of atomic zero-time computing steps, and very limited computational resources at gate-level make even simple operations prohibitively costly. In this paper, we introduce a modeling and analysis framework based on continuous computations and zero-bit message channels, and employ this framework for the correctness \&amp; performance analysis of a distributed fault-tolerant clocking approach for Systems-on-Chip (SoCs). Starting out from a "classic" distributed Byzantine fault-tolerant tick generation algorithm, we show how to adapt it for direct implementation in clockless digital logic, and rigorously prove its correctness and derive analytic expressions for worst case performance metrics like synchronization precision and clock frequency. Rather than on absolute delay values, both the algorithm's correctness and the achievable synchronization precision depend solely on the ratio of certain path delays. Since these ratios can be mapped directly to placement \&amp; routing constraints, there is typically no need for changing the algorithm when migrating to a faster implementation technology and/or when using a slightly different layout in an SoC.},
  issue_date = {January 2012},
  pagetotal = {33},
  keywords = {Clock synchronization,Fault-tolerant distributed systems,Modeling approaches,VLSI},
  file = {C:\Users\Felipe\Zotero\storage\PPQS9MIZ\Függer and Schmid - 2012 - Reconciling fault-tolerant distributed computing and systems-on-chip.pdf}
}

@inproceedings{greasley_using_2018,
  title = {Using Analytics with Discrete-Event Simulation},
  booktitle = {Proceedings of the 50th Computer Simulation Conference},
  author = {Greasley, Andrew},
  date = {2018},
  series = {{{SummerSim}} '18},
  publisher = {Society for Computer Simulation International},
  location = {San Diego, CA, USA},
  abstract = {Analytics is most often associated with the tools of statistics and data mining to transform big data into results for business decisions. This article aims to evaluate the role of the modelling tool of discrete-event simulation (DES) in conjunction with analytics. A review of articles which show the implementation of a DES in conjunction with big data, data mining, data farming, visual analytics and process mining is presented. A DES methodology is developed that provides a framework for the use of these techniques when conducting a DES study. A discussion is then presented of the challenges that are pertinent when undertaking DES in conjunction with analytics.},
  articleno = {13},
  pagetotal = {12},
  keywords = {analytics,discrete-event simulation,literature review},
  file = {C:\Users\Felipe\Zotero\storage\HPML8HGR\Greasley - 2018 - Using analytics with discrete-event simulation.pdf}
}

@inproceedings{gulay_integrated_2019,
  title = {Integrated {{Workflows}}: {{Generating Feedback Between Digital}} and {{Physical Realms}}},
  shorttitle = {Integrated {{Workflows}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Gulay, Emrecan and Lucero, Andrés},
  date = {2019-05-02},
  series = {{{CHI}} '19},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3290605.3300290},
  url = {https://dl.acm.org/doi/10.1145/3290605.3300290},
  urldate = {2025-07-08},
  abstract = {As design thinking shifted away from conventional methods with the rapid adoption of computer-aided design and fabrication technologies, architects have been seeking ways to initiate a comprehensive dialogue between the virtual and the material realms. Current methodologies do not offer embodied workflows that utilize the feedback obtained through a subsequent transition process between physical and digital design. Therefore, narrowing the separation between these two platforms remains as a research problem. This literature review elaborates the divide between physical and digital design, testing and manufacturing techniques in the morphological process of architectural form. We first review the digital transformation in the architectural design discourse. Then, we proceed by introducing a variety of methods that are integrating digital and physical workflows and suggesting an alternative approach. Our work unveils that there is a need for empirical research with a focus on integrated approaches to create intuitively embodied experiences for architectural designers.},
  isbn = {978-1-4503-5970-2},
  file = {C:\Users\Felipe\Zotero\storage\QEPJUY7L\Gulay and Lucero - 2019 - Integrated Workflows Generating Feedback Between Digital and Physical Realms.pdf}
}

@article{hazyuk_optimal_2017,
  title = {Optimal Design of Computer Experiments for Surrogate Models with Dimensionless Variables},
  author = {Hazyuk, Ion and Budinger, Marc and Sanchez, Florian and Gogu, Christian},
  date = {2017-09-01},
  journaltitle = {Structural and Multidisciplinary Optimization},
  shortjournal = {Struct Multidisc Optim},
  volume = {56},
  number = {3},
  pages = {663--679},
  issn = {1615-1488},
  doi = {10.1007/s00158-017-1683-7},
  url = {https://doi.org/10.1007/s00158-017-1683-7},
  urldate = {2024-07-16},
  abstract = {This paper presents a method for constructing optimal design of experiments (DoE) intended for building surrogate models using dimensionless (or non-dimensional) variables. In order to increase the fidelity of the model obtained by regression, the DoE needs to optimally cover the dimensionless space. However, in order to generate the data for the regression, one still needs a DoE for the physical variables, in order to carry out the simulations. Thus, there exist two spaces, each one needing a DoE. Since the dimensionless space is always smaller than the physical one, the challenge for building a DoE is that the relation between the two spaces is not bijective. Moreover, each space usually has its own domain constraints, which renders them not-surjective. This means that it is impossible to design the DoE in one space and then automatically generate the corresponding DoE in the other space while satisfying the constraints from both spaces. The solution proposed in the paper transforms the computation of the DoE into an optimization problem formulated in terms of a space-filling criterion (maximizing the minimum distance between neighboring points). An approach is proposed for efficiently solving this optimization problem in a two steps procedure. The method is particularly well suited for building surrogates in terms of dimensionless variables spanning several orders of magnitude (e.g. power laws). The paper also proposes some variations of the method; one when more control is needed on the number of levels on each non-dimensional variable and another one when a good distribution of the DoE is desired in the logarithmic scale. The DoE construction method is illustrated on three case studies. A purely numerical case illustrates each step of the method and two other, mechanical and thermal, case studies illustrate the results in different configurations and different practical aspects.},
  langid = {english},
  keywords = {Dimensional analysis,Non-dimensional variables,Optimal space filling,Optimization with space transformation,Surrogate modelling},
  file = {C\:\\Users\\Felipe\\Zotero\\storage\\6EBCZDW3\\Hazyuk et al. - 2017 - Optimal design of computer experiments for surroga.pdf;C\:\\Users\\Felipe\\Zotero\\storage\\DUJDR5BN\\Hazyuk et al. - 2017 - Optimal design of computer experiments for surrogate models with dimensionless variables.pdf;C\:\\Users\\Felipe\\Zotero\\storage\\V39P9DJI\\Hazyuk et al. - 2017 - Optimal design of computer experiments for surrogate models with dimensionless variables.pdf}
}

@article{hota_local-feature-based_2010,
  title = {Local-Feature-Based Image Retrieval with Weighted Relevance Feedback},
  author = {Hota, Rudra Narayan and Syed, Shahanaz and Krishna, P. Radha},
  date = {2010-10},
  journaltitle = {Int. J. Bus. Intell. Data Min.},
  volume = {5},
  number = {4},
  pages = {353--369},
  publisher = {Inderscience Publishers},
  location = {Geneva 15, CHE},
  issn = {1743-8195},
  doi = {10.1504/IJBIDM.2010.036124},
  url = {https://doi.org/10.1504/IJBIDM.2010.036124},
  abstract = {Accurate and fast retrieval of relevant images is a challenging task mainly due to the limitation in understanding hidden knowledge in images, known as semantic gap. In this work, we propose a novel approach which incorporates local feature representation for retrieval of grey and colour images from an archive with user intervention. We used histogram features, which are computationally efficient, hence resulting in quick image retrieval. The computed image feature vectors are used for similarity matching with weighted feed-backed image retrieval. We experimented both on publicly available and annotated image data sets to illustrate the effectiveness of our approach.},
  issue_date = {October 2010},
  pagetotal = {17},
  keywords = {accuracy,annotated data sets,annotations,archives,business intelligence,colour,computed images,data mining,extensible markup language,extraction,fast retrieval,feature representation,grey images,hidden knowledge,histograms,image retrieval,local features,multimedia,public data sets,querying,relevance,semantic gaps,similarity matching,speed,texture,user intervention,vectors,weighted feedback,XML streams}
}

@article{kumar_soft-checkpointing_2011,
  title = {Soft-Checkpointing Based Hybrid Synchronous Checkpointing Protocol for Mobile Distributed Systems},
  author = {Kumar, Parveen and Garg, Rachit},
  date = {2011-01},
  journaltitle = {Int. J. Distrib. Syst. Technol.},
  volume = {2},
  number = {1},
  pages = {1--13},
  publisher = {IGI Global},
  location = {USA},
  issn = {1947-3532},
  doi = {10.4018/jdst.2011010101},
  url = {https://doi.org/10.4018/jdst.2011010101},
  abstract = {Minimum-process coordinated checkpointing is a suitable approach to introduce fault tolerance in mobile distributed systems transparently. In order to balance the checkpointing overhead and the loss of computation on recovery, the authors propose a hybrid checkpointing algorithm, wherein an all-process coordinated checkpoint is taken after the execution of minimum-process coordinated checkpointing algorithm for a fixed number of times. In coordinated checkpointing, if a single process fails to take its checkpoint; all the checkpointing effort goes waste, because, each process has to abort its tentative checkpoint. In order to take the tentative checkpoint, an MH Mobile Host needs to transfer large checkpoint data to its local MSS over wireless channels. In this regard, the authors propose that in the first phase, all concerned MHs will take soft checkpoint only. Soft checkpoint is similar to mutable checkpoint. In this case, if some process fails to take checkpoint in the first phase, then MHs need to abort their soft checkpoints only. The effort of taking a soft checkpoint is negligibly small as compared to the tentative one. In the minimum-process coordinated checkpointing algorithm, an effort has been made to minimize the number of useless checkpoints and blocking of processes using probabilistic approach.},
  issue_date = {January 2011},
  pagetotal = {13},
  keywords = {Checkpoint,Consistent Global State,Coordinated Checkpointing and Mobile Systems,Fault Tolerance,Probabilistic Approach}
}

@article{lomotey_middleware_2018,
  title = {Middleware for Mobile Medical Data Management with Minimal Latency},
  author = {Lomotey, Richard K. and Deters, Ralph},
  date = {2018-12-01},
  journaltitle = {Information Systems Frontiers},
  shortjournal = {Inf Syst Front},
  volume = {20},
  number = {6},
  pages = {1281--1296},
  issn = {1572-9419},
  doi = {10.1007/s10796-016-9729-8},
  url = {https://doi.org/10.1007/s10796-016-9729-8},
  urldate = {2024-07-16},
  abstract = {A promise of mHealth is its capacity to facilitate the consumption of Electronic Health Record (EHR) data using mobile devices, which is central to promoting remote healthcare delivery. Our ongoing project, called SOPHRA, in collaboration with the City Hospital in Saskatoon, Canada, focuses on supporting care providers (e.g., physicians) in the collection/recording of medical data from patients remotely. A major challenge that needs to be addressed is the potential loss of communication between the mobile-clients and the health information system (HIS) during the transfer of the electronic health record (EHR). Overcoming this challenge will foster soft-real time medical data exchanges. However, the issue of communication loss can be as a result of the over-reliance on wireless networks such as Wi-Fi, which can sometimes be unstable. Thus, the goal of this work is to propose a mobile health (mHealth) architectural environment that can exploit the limited available bandwidth during the mobile medical data transfer. A middleware is proposed with the capability of managing different states of the medical data as the physicians updates the EHR. The work details the employment of the Bernoulli model as a means of determining and controlling different updates of the mHealth data for effective propagation.},
  langid = {english},
  keywords = {Cloud computing,Electronic health record,Middleware,Mobile computing,Mobile devices,Synchronization},
  file = {C:\Users\Felipe\Zotero\storage\W7K4PLIW\Lomotey and Deters - 2018 - Middleware for mobile medical data management with.pdf}
}

@inproceedings{luu_multiplatform_2015,
  title = {A Multiplatform Study of {{I}}/{{O}} Behavior on Petascale Supercomputers},
  booktitle = {Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing},
  author = {Luu, Huong and Winslett, Marianne and Gropp, William and Ross, Robert and Carns, Philip and Harms, Kevin and Prabhat, Mr and Byna, Suren and Yao, Yushu},
  date = {2015},
  series = {Hpdc '15},
  pages = {33--44},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2749246.2749269},
  url = {https://doi.org/10.1145/2749246.2749269},
  abstract = {We examine the I/O behavior of thousands of supercomputing applications "in the wild," by analyzing the Darshan logs of over a million jobs representing a combined total of six years of I/O behavior across three leading high-performance computing platforms. We mined these logs to analyze the I/O behavior of applications across all their runs on a platform; the evolution of an application's I/O behavior across time, and across platforms; and the I/O behavior of a platform's entire workload. Our analysis techniques can help developers and platform owners improve I/O performance and I/O system utilization, by quickly identifying underperforming applications and offering early intervention to save system resources. We summarize our observations regarding how jobs perform I/O and the throughput they attain in practice.},
  isbn = {978-1-4503-3550-8},
  pagetotal = {12},
  keywords = {hpc,input/output,parallel i/o,performance analysis},
  file = {C:\Users\Felipe\Zotero\storage\PMCBYLCW\Luu et al. - 2015 - A Multiplatform Study of IO Behavior on Petascale Supercomputers.pdf}
}

@article{nandy_study_2017,
  title = {A Study on Human Gait Dynamics: Modeling and Simulations on {{OpenSim}} Platform},
  shorttitle = {A Study on Human Gait Dynamics},
  author = {Nandy, Anup and Chakraborty, Pavan},
  date = {2017-10-01},
  journaltitle = {Multimedia Tools Appl.},
  volume = {76},
  number = {20},
  pages = {21365--21400},
  issn = {1380-7501},
  doi = {10.1007/s11042-016-4033-7},
  url = {https://doi.org/10.1007/s11042-016-4033-7},
  urldate = {2025-07-09},
  abstract = {The analysis of human gait dynamics allows an individual to obtain interesting biometric features through which gait disturbances can be observed from normal and abnormal gait patterns. The musculo-skeletal modeling of human movement helps to study the gait dynamics with expected simulations. It encourages the clinicians to identify the pathological gait for proper treatment. We have created a vision-based human locomotion laboratory to capture the healthy and non-healthy gait patterns. A low cost black uniform is taken to capture different person's gait data. This dress is made up with a piece of multiple color ribbon to identify different locations of body joints. It provides an added advantage during separation of different joint locations through color-based segmentation method. The demographic information of each subject helps to understand their gait dynamics. It instigates us to build a musculo-skeletal gait model on OpenSim simulation framework. The Regression Analysis (RA) technique is applied on crouch and healthy gait feature to measure the generalization ability and to uncover the gait profiles which are estimated by different error metrics such as, Root Mean Square Error (RMSE), Standard Deviation of Error (SDE), and Mean Error (ME). A computational method based on Normalized Auto Correlation (NAC) is computed to measure the gait disturbances in training subjects. The performance analysis of regression model on motion captured data has been validated with subject specific musculo-skeletal gait model on OpenSim platform.}
}

@online{noauthor_proceedings_nodate-1,
  title = {Proceedings of the 2016 {{ACM SIGPLAN International Conference}} on {{Object-Oriented Programming}}, {{Systems}}, {{Languages}}, and {{Applications}}},
  url = {https://dl.acm.org/doi/proceedings/10.1145/2983990},
  urldate = {2025-07-09},
  langid = {english},
  organization = {ACM Conferences},
  file = {C:\Users\Felipe\Zotero\storage\VXLXSRYC\2983990.html}
}

@online{noauthor_proceedings_nodate-2,
  title = {Proceedings of the 37th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  url = {https://dl.acm.org/doi/proceedings/10.1145/2908080},
  urldate = {2025-07-09},
  langid = {english},
  organization = {ACM Conferences},
  file = {C:\Users\Felipe\Zotero\storage\MRWKFJZ8\2908080.html}
}

@online{noauthor_proceedings_nodate-3,
  title = {Proceedings of the 43rd {{Annual ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  url = {https://dl.acm.org/doi/proceedings/10.1145/2837614},
  urldate = {2025-07-09},
  langid = {english},
  organization = {ACM Conferences},
  file = {C:\Users\Felipe\Zotero\storage\RRGRZAPA\2837614.html}
}

@online{noauthor_proceedings_nodate-4,
  title = {Proceedings of the 42nd {{Annual ACM SIGPLAN-SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  url = {https://dl.acm.org/doi/proceedings/10.1145/2676726},
  urldate = {2025-07-09},
  langid = {english},
  organization = {ACM Conferences},
  file = {C:\Users\Felipe\Zotero\storage\F89LRAXQ\2676726.html}
}

@online{noauthor_proceedings_nodate-5,
  title = {Proceedings of the 2015 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  url = {https://dl.acm.org/doi/proceedings/10.1145/2723372},
  urldate = {2025-07-09},
  langid = {english},
  organization = {ACM Conferences},
  file = {C:\Users\Felipe\Zotero\storage\ZFBWJJYD\2723372.html}
}

@article{numrich_computer_2014,
  title = {Computer Performance Analysis and the {{Pi Theorem}}},
  author = {Numrich, Robert W.},
  date = {2014-02-01},
  journaltitle = {Computer Science - Research and Development},
  shortjournal = {Comput. Sci.},
  volume = {29},
  number = {1},
  pages = {45--71},
  issn = {1865-2034},
  doi = {10.1007/s00450-010-0147-8},
  url = {https://doi.org/10.1007/s00450-010-0147-8},
  urldate = {2023-03-04},
  abstract = {This paper applies the Pi Theorem of dimensional analysis to a representative set of examples from computer performance analysis. It is a survey paper that takes a different look at problems involving latency, bandwidth, cache-miss ratios, and the efficiency of parallel numerical algorithms. The Pi Theorem is the fundamental tool of dimensional analysis, and it applies to problems in computer performance analysis just as well as it does to problems in other sciences. Applying it requires the definition of a system of measurement appropriate for computer performance analysis with a consistent set of units and dimensions. Then a straightforward recipe for each specific problem reduces the number of independent variables to a smaller number of dimensionless parameters. Two machines with the same values of these parameters are self-similar and behave the same way. Self-similarity relationships emphasize how machines are the same rather than how they are different. The Pi Theorem is simple to state and simple to prove, using purely algebraic methods, but the results that follow from it are often surprising and not simple at all. The results are often unexpected but they almost always reveal something new about the problem at hand.},
  keywords = {Bandwidth,Cache behavior,Computational force,Computational intensity,Dimensional analysis,Latency,Parallel algorithms,Performance analysis,Performance metrics,Pi Theorem,Scaling},
  note = {Cited By :2},
  file = {C\:\\Users\\Felipe\\Zotero\\storage\\6ZF4MKZW\\Numrich - 2014 - Computer performance analysis and the Pi Theorem.pdf;C\:\\Users\\Felipe\\Zotero\\storage\\EIH8XWAV\\display.html}
}

@article{numrich_self-similarity_2011,
  title = {Self-Similarity of Parallel Machines},
  author = {Numrich, Robert W. and Heroux, Michael A.},
  date = {2011-02-01},
  journaltitle = {Parallel Computing},
  shortjournal = {Parallel Computing},
  volume = {37},
  number = {2},
  pages = {69--84},
  issn = {0167-8191},
  doi = {10.1016/j.parco.2010.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0167819110001444},
  abstract = {Self-similarity is a property of physical systems that describes how to scale parameters such that dissimilar systems appear to be similar. Computer systems are self-similar if certain ratios of computational forces, also known as computational intensities, are equal. Two machines with different computational power, different network bandwidth and different inter-processor latency behave the same way if they have the same ratios of forces. For the parallel conjugate gradient algorithm studied in this paper, two machines are self-similar if and only if the ratio of one force describing latency effects to another force describing bandwidth effects is the same for both machines. For the two machines studied in this paper, this ratio, which we call the mixing coefficient, is invariant as problem size and processor count change. The two machines have the same mixing coefficient and belong to the same equivalence class.},
  keywords = {Benchmark analysis,Computational force,Computational intensity,Dimensional analysis,Equivalence class,Mixing coefficient,Parallel algorithms,Scaling,Self-similarity}
}

@inproceedings{planthaber_earthdb_2012,
  title = {{{EarthDB}}: Scalable Analysis of {{MODIS}} Data Using {{SciDB}}},
  booktitle = {Proceedings of the 1st {{ACM SIGSPATIAL}} International Workshop on Analytics for Big Geospatial Data},
  author = {Planthaber, Gary and Stonebraker, Michael and Frew, James},
  date = {2012},
  series = {{{BigSpatial}} '12},
  pages = {11--19},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2447481.2447483},
  url = {https://doi.org/10.1145/2447481.2447483},
  abstract = {Earth scientists are increasingly experiencing difficulties with analyzing rapidly growing volumes of complex data. Those who must perform analysis directly on low-level National Aeronautics and Space Administration (NASA) Moderate Resolution Imaging Spectroradiometer (MODIS) Level 1B calibrated and geolocated data, for example, encounter an arcane, high-volume data set that is burdensome to make use of. Instead, Earth scientists typically opt to use higher-level "canned" products provided by NASA. However, when these higher-level products fail to meet the requirements of a particular project, a cruel dilemma arises: cope with data products that don't exactly meet the project's needs or spend an enormous amount of resources extracting what is needed from the unadulterated low-level data. In this paper, we present EarthDB, a system that eliminates this dilemma by offering the following contributions:1. Enabling painless importing of MODIS Level 1B data into SciDB, a highly scalable science-oriented database platform that abstracts away the complexity of distributed storage and analysis of complex multi-dimensional data,2. Defining a schema that unifies storage and representation of MODIS Level 1B data, regardless of its source file,3. Supporting fast filtering and analysis of MODIS data through the use of an intuitive, high-level query language rather than complex procedural programming and,4. Providing the ability to easily define and reconfigure entire analysis pipelines within the SciDB database, allowing for rapid ad-hoc analysis. To demonstrate this ability, we provide sample benchmarks for the construction of true-color (RGB) and Normalized Difference Vegetative Index (NDVI) images from raw MODIS Level 1B data using relatively simple queries with scalable performance.},
  isbn = {978-1-4503-1692-7},
  pagetotal = {9},
  keywords = {analytics,big data,database,distributed,earth science,EarthDB,GIS,MODIS,NASA,remote sensing,SciDB},
  file = {C:\Users\Felipe\Zotero\storage\G9HVBYEA\Planthaber et al. - 2012 - EarthDB scalable analysis of MODIS data using SciDB.pdf}
}

@article{rahni_feasibility_2012,
  title = {Feasibility Analysis of Real-Time Transactions},
  author = {Rahni, Ahmed and Grolleau, Emmanuel and Richard, Michaël and Richard, Pascal},
  date = {2012-05},
  journaltitle = {Real-time Systems},
  shortjournal = {Real-Time Syst.},
  volume = {48},
  number = {3},
  pages = {320--358},
  publisher = {Kluwer Academic Publishers},
  location = {USA},
  issn = {0922-6443},
  doi = {10.1007/s11241-012-9147-z},
  url = {https://doi.org/10.1007/s11241-012-9147-z},
  abstract = {The objective of this paper is two-fold: give a survey of response time analysis (RTA), and contribute to schedulability analysis for the real-time transaction model. The RTA is studied under fixed priority policies (FPP), while schedulability analysis assumes an optimal scheduling algorithm (like the deadline driven scheduling algorithm EDF) in a preemptive context on uniprocessor systems. We compare the transaction model to the family of multiframe models, then present the exact, and approximated methods, as well as a tunable method to compute the RTA. Finally we present a new schedulability analysis method and an efficient algorithm to speed up this test.},
  issue_date = {May 2012},
  pagetotal = {39},
  keywords = {Deadline Driven Scheduling,Fixed Priority Policy,Processor demand,Real-time transactions,Response time analysis,Tasks with offsets}
}

@article{razia_sulthana_ontology_2019,
  title = {Ontology and Context Based Recommendation System Using {{Neuro-Fuzzy Classification}}},
  author = {Razia Sulthana, A. and Ramasamy, Subburaj},
  date = {2019-03-01},
  journaltitle = {Computers \& Electrical Engineering},
  shortjournal = {Computers \& Electrical Engineering},
  volume = {74},
  pages = {498--510},
  issn = {0045-7906},
  doi = {10.1016/j.compeleceng.2018.01.034},
  url = {https://www.sciencedirect.com/science/article/pii/S0045790617337382},
  abstract = {Recommendation Systems (RS) identify the products of likely interest to the user. In earlier studies, the recommendation systems classify the reviews of products as positive or negative, verbatim on the features contained in the reviews, without reference to their context. In this paper, we develop an Ontology and Context Based Recommendation System (OCBRS) to assess the context of and determine the opinion of the review. We propose a Neuro-Fuzzy Classification approach using fuzzy rules to extract the context of the review. This approach automatically classifies the reviews under the respective fuzzy rule. Ontology facilitates a hierarchical and systematic methodology to group the context and acts as repository of context. The proposed approach appears to improve the accuracy of the RS.},
  keywords = {Classification,Cold start,Context,Domain ontology,Features set,Neuro-fuzzy rules,Recommendation systems,Weightage matrix}
}

@article{roriz_junior_-line_2017,
  title = {An On-Line Algorithm for Cluster Detection of Mobile Nodes through Complex Event Processing},
  author = {Roriz Junior, Marcos and Endler, Markus and family=Silva, given=Francisco José da Silva, prefix=e, useprefix=false},
  date = {2017-03},
  journaltitle = {Inf. Syst.},
  volume = {64},
  number = {C},
  pages = {303--320},
  publisher = {Elsevier Science Ltd.},
  location = {GBR},
  issn = {0306-4379},
  doi = {10.1016/j.is.2015.12.003},
  url = {https://doi.org/10.1016/j.is.2015.12.003},
  abstract = {Clusters of mobile elements, such as vehicles and humans, are a common mobility pattern of interest for many applications. The on-line detection of them from large position streams of mobile entities is a challenging task because it requires algorithms that are capable of continuously and efficiently processing the high volume of position updates in a timely manner. Currently, the majority of approaches for cluster detection operate in batch mode, where position updates are recorded during time periods of certain length and then batch processed by an external routine, thus delaying the result of the cluster detection until the end of the time period. However, if the monitoring application requires results at a higher frequency than the one delivered by batch algorithms, then results might not reflect the current clustering state of the entities. To overcome this limitation, in this paper we propose DG2CEP, an algorithm that combines the well-known density-based clustering algorithm DBSCAN with the data stream processing paradigm Complex Event Processing (CEP) to achieve continuous, on-line detection of clusters. Our experiments with synthetic and real world datasets indicate that DG2CEP is able to detect the formation and dispersion of clusters with small latency and higher similarity to DBSCAN's output than batch-based approaches. HighlightsWe present an on-line algorithm (DG2CEP) for clustering position data streams.DG2CEP combines data stream mining with complex event processing concepts.DG2CEP can detect both the formation and the dispersion of clusters as data pass.Experimental results demonstrate that DG2CEP can rapidly detect cluster formations.Experiments also show that DG2CEP results are highly similar to off-line approaches (DBSCAN).},
  issue_date = {March 2017},
  pagetotal = {18},
  keywords = {Complex event processing,Data stream processing,Grid-based clustering,On-line stream clustering}
}

@inproceedings{roux_skier-ski_2010,
  title = {Skier-Ski System Model and Development of a Computer Simulation Aiming to Improve Skier's Performance and Ski},
  booktitle = {Proceedings of the 1st {{Augmented Human International Conference}}},
  author = {Roux, François and Dietrich, Gilles and Doix, Aude-Clémence},
  date = {2010-04-02},
  series = {{{AH}} '10},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1785455.1785468},
  url = {https://dl.acm.org/doi/10.1145/1785455.1785468},
  urldate = {2025-07-08},
  abstract = {Background. Based on personal experience of ski teaching, ski training and ski competing, we have noticed that some gaps exist between classical models describing body-techniques and actual motor acts made by performing athletes. The evolution of new parabolic shaped skis with new mechanical and geometric characteristics increase these differences even more. Generally, scientific research focuses on situations where skiers are separated from their skis. Also, many specialized magazines, handbooks and papers print articles with similar epistemology. In this paper, we describe the development of a three-dimensional analysis to model the skier-skis' system. We subsequently used the model to propose an evaluation template to coaches that includes eight techniques and three observable consequences in order to make objective evaluations of their athletes' body-techniques. Once the system is modeled, we can develop a computer simulation in the form of a jumping jack, respecting degrees of freedom of the model. We can manipulate movement of each body segment or skis' gears' characteristics to detect performance variations. The purpose of this project is to elaborate assumptions to improve performance and propose experimental protocols to coaches to enable them to evaluate performance. This computer simulation also involves board and wheeled sports.Methods. Eleven elite alpine skiers participated. Video cameras were used to observe motor acts in alpine skiers in two tasks: slalom and giant slalom turns. Kinematic data were input into the 3D Vision software. Two on-board balances were used to measure the six components of ski-boots→skis torques. All data sources were then synchronized.Findings. We found correlations between force and torque measurements, the progression of center of pressure and the eight body-techniques. Based on these results, we created a technological model of the skier-ski system. Then, we have made a reading template and a model to coach young alpine skiers in clubs and world cup alpine skiers and, we have obtained results demonstrating the usefulness of our research.Interpretation. These results suggest that it is now possible to create a three-dimensional simulator of an alpine skier. This tool is able to compare competitors' body-techniques to detect the most performing body-techniques. Additionally, it is potentially helpful to consider and evaluate new techniques and ski characteristics.},
  isbn = {978-1-60558-825-4},
  file = {C:\Users\Felipe\Zotero\storage\37M8SZPU\Roux et al. - 2010 - Skier-ski system model and development of a computer simulation aiming to improve skier's performanc.pdf}
}

@article{saadon_development_2020,
  title = {Development of Riverbank Erosion Rate Predictor for Natural Channels Using {{NARX-QR Factorization}} Model: A Case Study of {{Sg}}. {{Bernam}}, {{Selangor}}, {{Malaysia}}},
  shorttitle = {Development of Riverbank Erosion Rate Predictor for Natural Channels Using {{NARX-QR Factorization}} Model},
  author = {Saadon, Azlinda and Abdullah, Jazuri and Muhammad, Nur Shazwani and Ariffin, Junaidah},
  date = {2020-09-01},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {32},
  number = {18},
  pages = {14839--14849},
  issn = {1433-3058},
  doi = {10.1007/s00521-020-04835-5},
  url = {https://doi.org/10.1007/s00521-020-04835-5},
  urldate = {2024-07-16},
  abstract = {This study presents a novel and comprehensive model development technique to predict the riverbank erosion rate for a natural channel using a Nonlinear AutoRegressive model with eXogenous inputs and QR factorization parameter estimation, known as the NARX-QR Factorization model. The model was developed based on a 12-month extensive field measurement at Sg. Bernam. This study established the governing factors and derived dependent and independent variables for riverbank erosion using dimensional analysis, based on the Buckingham PI theorem. Two functional relationships were derived from dimensional analysis incorporating the factors governing riverbank erosion. The functional relationships include parameters of hydraulic characteristics of the channel, riverbank geometry and soil characteristics. Parameter estimation was conducted using a linear least squares technique to quantify riverbank erosion rates. The significant independent variables and fourteen models with several numbers of hidden layers were set as the input parameters to the NARX-QR Factorization model. The model performance analysis shows that Models 1 and 9, developed based on the proposed NARX-QR Factorization model, have the highest R2 at 75\% and 91\%, respectively. Model 1 performed the best with accuracies for training and testing datasets of 75\% and 73\%, respectively. Additionally, the scatter plot of Model 1 is uniformly distributed along the line of perfect agreement. Therefore, it is concluded that the NARX-QR Factorization model developed in this study performed well in estimating the riverbank erosion rate, particularly for a natural river similar to Sg. Bernam.},
  langid = {english},
  keywords = {Dimensionless analysis,NARX,Natural river,QR factorization,Riverbank erosion,Sensitivity analysis},
  file = {C:\Users\Felipe\Zotero\storage\YTJT7RCZ\Saadon et al. - 2020 - Development of riverbank erosion rate predictor fo.pdf}
}

@article{shen_stochastic_2015,
  title = {Stochastic Modeling of Dynamic Right-Sizing for Energy-Efficiency in Cloud Data Centers},
  author = {Shen, Dian and Luo, Junzhou and Dong, Fang and Fei, Xiang and Wang, Wei and Jin, Guoqing and Li, Weidong},
  date = {2015-07-01},
  journaltitle = {Special Section: Business and Industry Specific Cloud},
  shortjournal = {Future Generation Computer Systems},
  volume = {48},
  pages = {82--95},
  issn = {0167-739X},
  doi = {10.1016/j.future.2014.09.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X14001824},
  abstract = {Large data centers are usually built to support increasing computational and data storage demand of growing global business and industry, which consume an enormous amount of energy, at a huge cost to both business and the environment. However, much of that energy is wasted to maintain excess service capacity during periods of low load. In this paper, we investigate the problem of “right-sizing” data center for energy-efficiency through virtualization which allows consolidation of workloads into smaller number of servers while dynamically powering off the idle ones. In view of the dynamic nature of data centers, we propose a stochastic model based on Queueing theory to capture the main characteristics. Solving this model, we notice that there exists a tradeoff between the energy consumption and performance. We hereby develop a BFGS based algorithm to optimize the tradeoff by searching for the optimal system parameter values for the data center operators to “right-size” the data centers. We implement our Stochastic Right-sizing Model (SRM) and deploy it in the real-world cloud data center. Experiments with two real-world workload traces show that SRM can significantly reduce the energy consumption while maintaining high performance.},
  keywords = {Cloud computing,Data center,Energy-efficiency,Queueing theory,Virtualization}
}

@inproceedings{tang_frag-shells_2017,
  title = {Frag-Shells Cube Based on Hierarchical Dimension Encoding Tree},
  booktitle = {Proceedings of the 11th {{International Conference}} on {{Ubiquitous Information Management}} and {{Communication}}},
  author = {Tang, Shanshan and Wan, Dingsheng and Zhu, Yuelong and Yao, Jianguo},
  date = {2017-01-05},
  series = {{{IMCOM}} '17},
  pages = {1--9},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3022227.3022229},
  url = {https://dl.acm.org/doi/10.1145/3022227.3022229},
  urldate = {2025-07-08},
  abstract = {Pre-computation of data cube can greatly improve the performance of OLAP (online analytical processing). There are a lot of effective pre-computation methods of data cube. But in practice, appropriate pre-computation method for the characteristics of data set plays a crucial role in improving the efficiency of data cube pre-computation.In view of the high-dimensional and hierarchical dimension of water census data characteristic, Frag-Shells cube based on hierarchical dimension encoding tree has been proposed in this paper. In order to improve the retrieval efficiency and reduce redundant information in hierarchical dimensions, we have proposed hierarchical dimension encoding tree (HDE-Tree) to index the hierarchical dimension in Frag-Shells cube. In order to increase the efficiency of cube construction, improved Frag-Shells cube calculation method has been used to compute the tuples of non-hierarchical dimension fragments. In order to compress the size of data cube, the TID-List compression method has been used to decrease the storage cost of inverted index in each tuple. Experiments show that the Frag-Shells cube based on hierarchical dimension encoding tree can reduce the construction time and storage cost of data cube which has high-dimensional and dimensions hierarchical figures.},
  isbn = {978-1-4503-4888-1},
  file = {C:\Users\Felipe\Zotero\storage\XTFF9YTZ\Tang et al. - 2017 - Frag-shells cube based on hierarchical dimension encoding tree.pdf}
}

@article{teng_scalable_2016,
  title = {Scalable Algorithms for Data and Network Analysis},
  author = {Teng, Shang-Hua},
  date = {2016-05},
  journaltitle = {Found. Trends Theor. Comput. Sci.},
  volume = {12},
  number = {1--2},
  pages = {1--274},
  publisher = {Now Publishers Inc.},
  location = {Hanover, MA, USA},
  issn = {1551-305X},
  doi = {10.1561/0400000051},
  url = {https://doi.org/10.1561/0400000051},
  abstract = {In the age of Big Data, efficient algorithms are now in higher demand more than ever before. While Big Data takes us into the asymptotic world envisioned by our pioneers, it also challenges the classical notion of efficient algorithms: Algorithms that used to be considered efficient, according to polynomial-time characterization, may no longer be adequate for solving today’s problems. It is not just desirable, but essential, that efficient algorithms should be scalable. In other words, their complexity should be nearly linear or sub-linear with respect to the problem size. Thus, scalability, not just polynomial-time computability, should be elevated as the central complexity notion for characterizing efficient computation. In this tutorial, I will survey a family of algorithmic techniques for the design of provably-good scalable algorithms. These techniques include local network exploration, advanced sampling, sparsification, and geometric partitioning. They also include spectral graph-theoretical methods, such as those used for computing electrical flows and sampling from Gaussian Markov random fields. These methods exemplify the fusion of combinatorial, numerical, and statistical thinking in network analysis. I will illustrate the use of these techniques by a few basic problems that are fundamental in network analysis, particularly for the identification of significant nodes and coherent clusters/communities in social and information networks. I also take this opportunity to discuss some frameworks beyond graph-theoretical models for studying conceptual questions to understand multifaceted network data that arise in social influence, network dynamics, and Internet economics.},
  issue_date = {May 2016},
  pagetotal = {278}
}

@article{thunis_tool_2012,
  title = {A Tool to Evaluate Air Quality Model Performances in Regulatory Applications},
  author = {Thunis, Philippe and Georgieva, Emilia and Pederzoli, Anna},
  date = {2012-12-01},
  journaltitle = {Environ. Model. Softw.},
  volume = {38},
  pages = {220--230},
  issn = {1364-8152},
  doi = {10.1016/j.envsoft.2012.06.005},
  url = {https://doi.org/10.1016/j.envsoft.2012.06.005},
  urldate = {2025-07-09},
  abstract = {This paper describes the details of the DELTA Tool and Benchmarking service for air quality models, recently developed in the framework of FAIRMODE (Forum for Air Quality Modelling in Europe). One of the main objectives of the FAIRMODE activities is the development of a procedure for the evaluation and benchmarking of air quality modelling applications for regulatory purposes. The DELTA Tool is a specific software which provides summary statistics (i.e. BIAS, RMSE, correlation coefficient) as well as scatter-plots, time series plots, Taylor, Target and other diagrams providing an overview of the quality of model results with respect to monitored data. Moreover, the benchmarking service implemented in DELTA produces summary reports containing performance indicators related to a given model application in the frame of the EU Air Quality Directive (AQD, 2008). This work describes the structure of the DELTA tool and template for reporting model performances. Some examples of application are also briefly presented.}
}

@article{wolny_thirteen_2020,
  title = {Thirteen Years of {{SysML}}: A Systematic Mapping Study},
  shorttitle = {Thirteen Years of {{SysML}}},
  author = {Wolny, Sabine and Mazak, Alexandra and Carpella, Christine and Geist, Verena and Wimmer, Manuel},
  date = {2020-01-01},
  journaltitle = {Software and Systems Modeling},
  shortjournal = {Softw Syst Model},
  volume = {19},
  number = {1},
  pages = {111--169},
  issn = {1619-1374},
  doi = {10.1007/s10270-019-00735-y},
  url = {https://doi.org/10.1007/s10270-019-00735-y},
  urldate = {2024-07-16},
  abstract = {The OMG standard Systems Modeling Language (SysML) has been on the market for about thirteen years. This standard is an extended subset of UML providing a graphical modeling language for designing complex systems by considering software as well as hardware parts. Over the period of thirteen years, many publications have covered various aspects of SysML in different research fields. The aim of this paper is to conduct a systematic mapping study about SysML to identify the different categories of papers, (i) to get an overview of existing research topics and groups, (ii) to identify whether there are any publication trends, and (iii) to uncover possible missing links. We followed the guidelines for conducting a systematic mapping study by Petersen et al. (Inf Softw Technol 64:1–18, 2015) to analyze SysML publications from 2005 to 2017. Our analysis revealed the following main findings: (i) there is a growing scientific interest in SysML in the last years particularly in the research field of Software Engineering, (ii) SysML is mostly used in the design or validation phase, rather than in the implementation phase, (iii) the most commonly used diagram types are the SysML-specific requirement diagram, parametric diagram, and block diagram, together with the activity diagram and state machine diagram known from UML, (iv) SysML is a specific UML profile mostly used in systems engineering; however, the language has to be customized to accommodate domain-specific aspects, (v) related to collaborations for SysML research over the world, there are more individual research groups than large international networks. This study provides a solid basis for classifying existing approaches for SysML. Researchers can use our results (i) for identifying open research issues, (ii) for a better understanding of the state of the art, and (iii) as a reference for finding specific approaches about SysML.},
  langid = {english},
  keywords = {SysML,Systematic mapping study,Systems engineering},
  file = {C:\Users\Felipe\Zotero\storage\6SDGHCN3\Wolny et al. - 2020 - Thirteen years of SysML a systematic mapping stud.pdf}
}

@article{yang_designing_2010,
  title = {Designing an Effective {{P2P}} System for a {{VoD}} System to Exploit the Multicast Communication},
  author = {Yang, X.Y. and Cores, F. and Hernández, P. and Ripoll, A. and Luque, E.},
  date = {2010-12-01},
  journaltitle = {Journal of Parallel and Distributed Computing},
  shortjournal = {Journal of Parallel and Distributed Computing},
  volume = {70},
  number = {12},
  pages = {1175--1192},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2010.07.007},
  url = {https://www.sciencedirect.com/science/article/pii/S074373151000136X},
  abstract = {A distributed video-on-demand system~(DVoD) with multiple server-nodes is a cost-effective and fault-tolerant solution for a high scalable enterprise video-on-demand (VoD) system. However, such a server-oriented design is highly vulnerable to workload variations given that the service capacity is limited. Peer-to-Peer (P2P) has been introduced as an architectural solution with self-growing capacity. However, the characteristics of a pure P2P system such as the peer transient nature and high network overhead make this kind of architecture unsuitable for a fully interactive VoD system. In this paper, we propose a new efficient integrated VoD architecture, called DPn2Pm, that combines DVoD with a P2P system and multicast communications. The DVoD’s server-nodes provide a minimum required quality of service (QoS) and the P2P system is able to offer the mechanism to increase the system service capacity according to client demands. Multicast communication, wherever it is possible, is effectively exploited by our P2P system. In our design, each client is able to send video information to a set of m clients using only one multicast channel. Furthermore, the collaboration mechanism is able to coordinate a set of clients to create one collaboration group to replace the server, providing an extensive, efficient and low network-overhead collaboration mechanism from n-peers to m-peers. Regardless of the video the client is watching, our P2P scheme allows every active client to collaborate with the server. The P2P scheme is complemented with recovery mechanisms that are able to replace the failed client before affecting the QoS, offering continuous playback. The proposed approach has been broadly evaluated, firstly using a mathematical model to derive the theoretical performance and secondly using a simulation environment to analyze the system’s dynamic behavior, the VCR interaction impact and the client failures. Comparing DPn2Pm with other DVoD architectures and the most relevant P2P delivery policies, we show that our design is an improvement on previous solutions, providing a higher scalability.},
  keywords = {Content delivery network,Distributed VoD,Multicast,P2P,Proxy,Video-on-demand},
  file = {C:\Users\Felipe\Zotero\storage\E5CPS55F\Yang et al. - 2010 - Designing an effective P2P system for a VoD system.pdf}
}
